{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Este es el ejemplo 4.3. Problema del Apostador (Gambler’s Problem) del libro de Sutton. \n",
    "\n",
    "Un apostador tiene la oportunidad de hacer apuestas a los resultados de una secuencia de tiros de una moneda. \n",
    "Si la moneda cae cara, gana tantos dólares como apostó en esa tirada.\n",
    "Si la moneda cae ceca, pierde lo apostado. El juego termina cuando un apostador gana alcanzando su objetivo de $100, o pierde quedándose sin dinero.\n",
    "\n",
    "En cada tirada el apostador debe decidir qué porción de su capital quiere apostar, una cantidad entera de dólares.\n",
    "El problema puede ser formulado como un MDP finito sin descuento, episódico.\n",
    "\n",
    "Los estados posibles del capital del apostador son: s ∈ {1, 2, . . . , 99}.\n",
    "\n",
    "La acciones son apuestas,  a ∈ {0, 1, . . . , min(s, 100 − s)}. \n",
    "\n",
    "La recompensa es cero en todas las transiciones excepto en aquellas en que el apostador alcanza su objetivo, en que la recompensa es +1.\n",
    "\n",
    "La función de estado-valor da la probabilidad de ganar desde cada estado. Una política es una función de niveles de capital a apuestas. La política óptima maximiza la probabilidad de obtener el objetivo. Llamemos p_h la probabilidad de que una moneda salga cara. Si p_h es conocida, entonces el problema se puede resolver, por ejemplo, con iteración de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Implementar iteración de valor para el problema del apostador y resolverlo para p_h = 0.25 y p_h = 0.55.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration_for_gamblers(p_h, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p_h: Probabilidad de que una moneda caiga cara\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(s, V, rewards):\n",
    "        \"\"\"\n",
    "        Función auxiliar que calcula el valor de todas las acciones dado un estado.\n",
    "        \n",
    "        Args:\n",
    "            s: El capital del apostador. Entero.\n",
    "            V: El vector que contiene los valores en cada estado.\n",
    "            rewards: El vector recompensa.\n",
    "                        \n",
    "        Returns:\n",
    "            Un vector que contiene el valor esperado de cada acción.\n",
    "            Su longitud es igual a la cantidad de acciones.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implementar!\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    # Implementar!\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy, v = value_iteration_for_gamblers(0.25)\n",
    "\n",
    "print(\"Política optimizada:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Función de valor óptima:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = value_iteration_for_gamblers(0.55)\n",
    "\n",
    "print(\"Política optimizada:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Función de valor óptima:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotear la política final (apuesta) vs estado (capital)\n",
    "\n",
    "# Implementar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotear capital vs política final\n",
    "\n",
    "# Implementar!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad\n",
    "En esta zona voy a ir armando los componentes necesarios para poder resolver el ejercicio, para luego pasarlo en limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constantes\n",
    "n_S = 101\n",
    "n_A = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empezamos con una random\n",
    "\n",
    "def policy_random(n_S=101, n_A=51):\n",
    "    policy = np.zeros([n_S, n_A])\n",
    "    \n",
    "    for s in range(51):\n",
    "        for a in range(0, s+1):\n",
    "            policy[s][a] = 1/(s+1)\n",
    "\n",
    "    for s in range(51,101):\n",
    "        n_a_posibles = min(s,100-s)+1\n",
    "        for a in range(n_a_posibles):\n",
    "            policy[s][a] = 1/(n_a_posibles)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion de valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V = np.zeros(n_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se respeta el formato utilizado en el ejercicio anterior, cada env[s][a] = lista de tuplas\n",
    "# con formato (probabilidad, sig_estado, reward, done)\n",
    "\n",
    "def reset_env(p=0.25):\n",
    "    env = {}\n",
    "\n",
    "    for s in range(0, 101):\n",
    "        env[s] = {}\n",
    "\n",
    "        for a in range(min(s,100-s)+1):        \n",
    "            env[s][a] = []\n",
    "\n",
    "            if (a==0):\n",
    "                r = 1.0 if (s==100) else 0.0\n",
    "                done = True if ((s==0) or (s==100)) else False\n",
    "                env[s][a] = [(1.0, s, r, done)]\n",
    "            else:\n",
    "                sig_estado_si_gano = s+a\n",
    "                sig_estado_si_pierdo = s-a\n",
    "                done_si_gano = True if (sig_estado_si_gano==100) else False\n",
    "                done_si_pierdo = True if (sig_estado_si_pierdo==0) else False\n",
    "                r = 1.0 if (sig_estado_si_gano==100) else 0.0\n",
    "                env[s][a] = [(p, sig_estado_si_gano, r, done_si_gano), ((1-p), (s-a), 0.0, done_si_pierdo)]\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_eval\n",
    "\n",
    "def policy_eval(policy, env, n_S=101, discount_factor=1.0, theta=1e-05, n_max = 0):\n",
    "    \n",
    "    pasos = 0\n",
    "    V = np.zeros(n_S)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(1, n_S-1):\n",
    "            v = 0\n",
    "            for a in env[s]:\n",
    "                for  prob, next_state, reward, done in env[s][a]:\n",
    "                    v += policy[s][a] * prob * (reward + discount_factor * V[next_state])\n",
    "            \n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            \n",
    "            # for debug:\n",
    "            #print(f\"Paso {pasos+1} | Estado {s} | V_previo = {V[s]}, V_nuevo = {v}\")\n",
    "            \n",
    "            \n",
    "            V[s] = v\n",
    "                \n",
    "        pasos += 1\n",
    "        \n",
    "        if delta < theta:\n",
    "            termino_correctamente = True\n",
    "            break\n",
    "            \n",
    "        if (n_max):\n",
    "            if (pasos == n_max):\n",
    "                termino_correctamente = False\n",
    "                break\n",
    "    \n",
    "    if (termino_correctamente):\n",
    "        print(f\"La eval. termino en {pasos} pasos.\")\n",
    "    else:\n",
    "        print(f\"Se alcanzaron los pasos maximos.\")\n",
    "   \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, tetha=1e-08, n_S=101, policy_eval_fn=policy_eval, policy_seed_fn=policy_random, discount_factor=1.0, n_max=0, debug=False):\n",
    "    \n",
    "    pasos = 0\n",
    "    \n",
    "    # Comenzar con política aleatoria\n",
    "    policy = policy_seed_fn()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "                \n",
    "        V = policy_eval_fn(policy, env, n_S=n_S, discount_factor=discount_factor)\n",
    "        \n",
    "            \n",
    "        politica_estable = True\n",
    "        \n",
    "        for s in range(1, n_S-1):\n",
    "                        \n",
    "            a_mas_probable = policy[s].tolist().index(max(policy[s]))\n",
    "            # OJO, hay que pensar una manera de que cuando las p sean todas iguales, la cambie por 1.\n",
    "            \n",
    "            #print(f\"state = {s}, a mas probable = {a_mas_probable}\")\n",
    "            \n",
    "            acciones = []        \n",
    "            for a in env[s]:\n",
    "                retorno_a = 0\n",
    "                for transicion in env[s][a]:\n",
    "                    p_transicion = transicion[0]\n",
    "                    proximo_estado = transicion[1]\n",
    "                    reward = transicion[2]\n",
    "                    retorno_a += p_transicion*(reward + discount_factor*V[proximo_estado])\n",
    "                acciones.append((a, retorno_a))\n",
    "\n",
    "\n",
    "            mejor_a = max(acciones, key=lambda x: x[1])[0]\n",
    "\n",
    "            \n",
    "            # si la acción de la política actual no coincide con la mejor calculada\n",
    "            # actualizar la política\n",
    "            # marcar que la política no fue estable en este paso\n",
    "            diferencia = acciones[mejor_a][1] - acciones[a_mas_probable][1]\n",
    "            #if ((diferencia > tetha) and (mejor_a != 0)):\n",
    "                    \n",
    "            if (diferencia > tetha):\n",
    "                if (debug):\n",
    "                    print(f\"paso {pasos+1} | s = {s} | a_priori = {a_mas_probable} | dsp = {mejor_a}\")\n",
    "                    print(f\"a_priori p = {acciones[a_mas_probable][1]} | dsp p = {acciones[mejor_a][1]}\")\n",
    "                politica_estable = False\n",
    "                pol_s = np.zeros(51)\n",
    "                pol_s[mejor_a] = 1\n",
    "                policy[s] = pol_s\n",
    "   \n",
    "        # si la política es estable, devolver la política óptima y la función de valor de esa política\n",
    "        if (politica_estable):\n",
    "            print(\"Se encontro una politica optima.\")\n",
    "            return policy, V\n",
    "        \n",
    "        pasos += 1\n",
    "        \n",
    "        if (n_max):\n",
    "            if (pasos == n_max):\n",
    "                print(\"Se corta la optimizacion de la politica al llegar a los pasos maximos.\")\n",
    "                return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeamos si funciona..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La eval. termino en 61 pasos.\n",
      "La eval. termino en 145 pasos.\n",
      "La eval. termino en 418 pasos.\n",
      "La eval. termino en 686 pasos.\n",
      "La eval. termino en 850 pasos.\n",
      "La eval. termino en 994 pasos.\n",
      "La eval. termino en 1042 pasos.\n",
      "La eval. termino en 1113 pasos.\n",
      "La eval. termino en 1117 pasos.\n",
      "Se encontro una politica optima.\n"
     ]
    }
   ],
   "source": [
    "env = reset_env(p=0.55)\n",
    "policy, v = policy_improvement(env, n_max=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si tenes 1 dls, tenes que apostar 1. Tenes una prob de ganar del 18.16%.\n",
      "Si tenes 2 dls, tenes que apostar 1. Tenes una prob de ganar del 33.01%.\n",
      "Si tenes 3 dls, tenes que apostar 1. Tenes una prob de ganar del 45.17%.\n",
      "Si tenes 4 dls, tenes que apostar 1. Tenes una prob de ganar del 55.12%.\n",
      "Si tenes 5 dls, tenes que apostar 1. Tenes una prob de ganar del 63.26%.\n",
      "Si tenes 6 dls, tenes que apostar 1. Tenes una prob de ganar del 69.92%.\n",
      "Si tenes 7 dls, tenes que apostar 1. Tenes una prob de ganar del 75.37%.\n",
      "Si tenes 8 dls, tenes que apostar 1. Tenes una prob de ganar del 79.83%.\n",
      "Si tenes 9 dls, tenes que apostar 1. Tenes una prob de ganar del 83.48%.\n",
      "Si tenes 10 dls, tenes que apostar 1. Tenes una prob de ganar del 86.46%.\n",
      "Si tenes 11 dls, tenes que apostar 1. Tenes una prob de ganar del 88.91%.\n",
      "Si tenes 12 dls, tenes que apostar 1. Tenes una prob de ganar del 90.91%.\n",
      "Si tenes 13 dls, tenes que apostar 1. Tenes una prob de ganar del 92.55%.\n",
      "Si tenes 14 dls, tenes que apostar 1. Tenes una prob de ganar del 93.89%.\n",
      "Si tenes 15 dls, tenes que apostar 1. Tenes una prob de ganar del 94.99%.\n",
      "Si tenes 16 dls, tenes que apostar 1. Tenes una prob de ganar del 95.89%.\n",
      "Si tenes 17 dls, tenes que apostar 1. Tenes una prob de ganar del 96.63%.\n",
      "Si tenes 18 dls, tenes que apostar 1. Tenes una prob de ganar del 97.23%.\n",
      "Si tenes 19 dls, tenes que apostar 1. Tenes una prob de ganar del 97.72%.\n",
      "Si tenes 20 dls, tenes que apostar 1. Tenes una prob de ganar del 98.13%.\n",
      "Si tenes 21 dls, tenes que apostar 1. Tenes una prob de ganar del 98.46%.\n",
      "Si tenes 22 dls, tenes que apostar 1. Tenes una prob de ganar del 98.73%.\n",
      "Si tenes 23 dls, tenes que apostar 1. Tenes una prob de ganar del 98.96%.\n",
      "Si tenes 24 dls, tenes que apostar 1. Tenes una prob de ganar del 99.14%.\n",
      "Si tenes 25 dls, tenes que apostar 1. Tenes una prob de ganar del 99.29%.\n",
      "Si tenes 26 dls, tenes que apostar 1. Tenes una prob de ganar del 99.42%.\n",
      "Si tenes 27 dls, tenes que apostar 1. Tenes una prob de ganar del 99.52%.\n",
      "Si tenes 28 dls, tenes que apostar 1. Tenes una prob de ganar del 99.6%.\n",
      "Si tenes 29 dls, tenes que apostar 1. Tenes una prob de ganar del 99.67%.\n",
      "Si tenes 30 dls, tenes que apostar 1. Tenes una prob de ganar del 99.73%.\n",
      "Si tenes 31 dls, tenes que apostar 1. Tenes una prob de ganar del 99.77%.\n",
      "Si tenes 32 dls, tenes que apostar 1. Tenes una prob de ganar del 99.81%.\n",
      "Si tenes 33 dls, tenes que apostar 1. Tenes una prob de ganar del 99.84%.\n",
      "Si tenes 34 dls, tenes que apostar 1. Tenes una prob de ganar del 99.87%.\n",
      "Si tenes 35 dls, tenes que apostar 1. Tenes una prob de ganar del 99.89%.\n",
      "Si tenes 36 dls, tenes que apostar 1. Tenes una prob de ganar del 99.91%.\n",
      "Si tenes 37 dls, tenes que apostar 1. Tenes una prob de ganar del 99.92%.\n",
      "Si tenes 38 dls, tenes que apostar 1. Tenes una prob de ganar del 99.94%.\n",
      "Si tenes 39 dls, tenes que apostar 1. Tenes una prob de ganar del 99.95%.\n",
      "Si tenes 40 dls, tenes que apostar 1. Tenes una prob de ganar del 99.95%.\n",
      "Si tenes 41 dls, tenes que apostar 1. Tenes una prob de ganar del 99.96%.\n",
      "Si tenes 42 dls, tenes que apostar 1. Tenes una prob de ganar del 99.97%.\n",
      "Si tenes 43 dls, tenes que apostar 1. Tenes una prob de ganar del 99.97%.\n",
      "Si tenes 44 dls, tenes que apostar 1. Tenes una prob de ganar del 99.98%.\n",
      "Si tenes 45 dls, tenes que apostar 1. Tenes una prob de ganar del 99.98%.\n",
      "Si tenes 46 dls, tenes que apostar 1. Tenes una prob de ganar del 99.98%.\n",
      "Si tenes 47 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 48 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 49 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 50 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 51 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 52 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 53 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 54 dls, tenes que apostar 1. Tenes una prob de ganar del 99.99%.\n",
      "Si tenes 55 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 56 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 57 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 58 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 59 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 60 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 61 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 62 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 63 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 64 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 65 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 66 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 67 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 68 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 69 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 70 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 71 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 72 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 73 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 74 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 75 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 76 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 77 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 78 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 79 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 80 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 81 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 82 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 83 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 84 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 85 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 86 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 87 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 88 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 89 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 90 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 91 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 92 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 93 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 94 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 95 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 96 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 97 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 98 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n",
      "Si tenes 99 dls, tenes que apostar 1. Tenes una prob de ganar del 100.0%.\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for estado in policy:\n",
    "    idx = estado.tolist().index(max(estado))\n",
    "    if ((s != 0) and (s != 100)):\n",
    "        print(f\"Si tenes {s} dls, tenes que apostar {idx}. Tenes una prob de ganar del {round(100*v[s], 2)}%.\")\n",
    "    s+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
