{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Este es el ejemplo 4.3. Problema del Apostador (Gambler’s Problem) del libro de Sutton. \n",
    "\n",
    "Un apostador tiene la oportunidad de hacer apuestas a los resultados de una secuencia de tiros de una moneda. \n",
    "Si la moneda cae cara, gana tantos dólares como apostó en esa tirada.\n",
    "Si la moneda cae ceca, pierde lo apostado. El juego termina cuando un apostador gana alcanzando su objetivo de $100, o pierde quedándose sin dinero.\n",
    "\n",
    "En cada tirada el apostador debe decidir qué porción de su capital quiere apostar, una cantidad entera de dólares.\n",
    "El problema puede ser formulado como un MDP finito sin descuento, episódico.\n",
    "\n",
    "Los estados posibles del capital del apostador son: s ∈ {1, 2, . . . , 99}.\n",
    "\n",
    "La acciones son apuestas,  a ∈ {0, 1, . . . , min(s, 100 − s)}. \n",
    "\n",
    "La recompensa es cero en todas las transiciones excepto en aquellas en que el apostador alcanza su objetivo, en que la recompensa es +1.\n",
    "\n",
    "La función de estado-valor da la probabilidad de ganar desde cada estado. Una política es una función de niveles de capital a apuestas. La política óptima maximiza la probabilidad de obtener el objetivo. Llamemos p_h la probabilidad de que una moneda salga cara. Si p_h es conocida, entonces el problema se puede resolver, por ejemplo, con iteración de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Implementar iteración de valor para el problema del apostador y resolverlo para p_h = 0.25 y p_h = 0.55.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(s, V, rewards):\n",
    "        \"\"\"\n",
    "        Función auxiliar que calcula el valor de todas las acciones dado un estado.\n",
    "        \n",
    "        Args:\n",
    "            s: El capital del apostador. Entero.\n",
    "            V: El vector que contiene los valores en cada estado.\n",
    "            rewards: El vector recompensa.\n",
    "                        \n",
    "        Returns:\n",
    "            Un vector que contiene el valor esperado de cada acción.\n",
    "            Su longitud es igual a la cantidad de acciones.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implementar!\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_for_gamblers(p_h, theta_eval=1e-10, discount_factor=1.0, debug=0,\n",
    "                                 one_step_lookahead_fn=one_step_lookahead, max_steps_eval=0, max_steps_opt=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p_h: Probabilidad de que una moneda caiga cara\n",
    "        theta_eval: theta para la evaluacion, diferencia entre V_k(s) y V_k+1(s)\n",
    "        theta_opt: theta para la optimizacion, diferencia relativa entre la mejor accion y la mas probable.\n",
    "        discount_factor:\n",
    "        debug:  {0, no debuggea nada.\n",
    "                 1, debug parcial.\n",
    "                 2, debug total}\n",
    "        one_step_lookahead_fn: funcion encargada de obtener el valor esperado de cada accion.\n",
    "        max_steps_eval: maxima cantidad de pasos a iterar en cada evaluacion.\n",
    "        max_steps_opt: maxima cantidad de pasos a iterar en la optimizacion de politica.\n",
    "    \"\"\"\n",
    "     \n",
    "    # constantes\n",
    "    n_S = 101\n",
    "    n_A = 51\n",
    "    \n",
    "    # definimos una politica random para iniciar\n",
    "    policy = np.zeros([n_S, n_A])\n",
    "    \n",
    "    for s in range(51):\n",
    "        for a in range(0, s+1):\n",
    "            policy[s][a] = 1/(s+1)\n",
    "\n",
    "    for s in range(51,101):\n",
    "        n_a_posibles = min(s,100-s)+1\n",
    "        for a in range(n_a_posibles):\n",
    "            policy[s][a] = 1/(n_a_posibles)\n",
    "         \n",
    "    # definimos nuestro env a mano\n",
    "    env = {}\n",
    "\n",
    "    for s in range(0, 101):\n",
    "        env[s] = {}\n",
    "\n",
    "        for a in range(min(s,100-s)+1):        \n",
    "            env[s][a] = []\n",
    "\n",
    "            if (a==0):\n",
    "                r = 1.0 if (s==100) else 0.0\n",
    "                done = True if ((s==0) or (s==100)) else False\n",
    "                env[s][a] = [(1.0, s, r, done)]\n",
    "            else:\n",
    "                sig_estado_si_gano = s+a\n",
    "                sig_estado_si_pierdo = s-a\n",
    "                done_si_gano = True if (sig_estado_si_gano==100) else False\n",
    "                done_si_pierdo = True if (sig_estado_si_pierdo==0) else False\n",
    "                r = 1.0 if (sig_estado_si_gano==100) else 0.0\n",
    "                env[s][a] = [(p_h, sig_estado_si_gano, r, done_si_gano), ((1-p_h), (s-a), 0.0, done_si_pierdo)]\n",
    "    \n",
    "    \n",
    "    # inicio del algoritmo iterativo de optimizacion\n",
    "    paso = 1\n",
    "\n",
    "    while True:\n",
    "        if (debug):\n",
    "            if (paso!=1):\n",
    "                print(f\"\\nINICIO DEL PASO {paso}\")\n",
    "            else:\n",
    "                print(f\"INICIO DEL PASO {paso}\")\n",
    "        \n",
    "        # policy evaluation\n",
    "        pasos_eval = 0\n",
    "        V = np.zeros(n_S)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            delta = 0\n",
    "            for s in range(1, n_S-1):\n",
    "                v = 0\n",
    "                for a in env[s]:\n",
    "                    for  prob, next_state, reward, done in env[s][a]:\n",
    "                        v += policy[s][a] * prob * (reward + discount_factor * V[next_state])\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "                # for debug:\n",
    "                #print(f\"Paso {pasos+1} | Estado {s} | V_previo = {V[s]}, V_nuevo = {v}\")\n",
    "\n",
    "                V[s] = v  \n",
    "            pasos_eval += 1\n",
    "\n",
    "            if delta < theta_eval:\n",
    "                termino_correctamente = True\n",
    "                break\n",
    "\n",
    "            if (max_steps_eval):\n",
    "                if (pasos_eval == max_steps_eval):\n",
    "                    termino_correctamente = False\n",
    "                    break\n",
    "\n",
    "        if (debug):\n",
    "            if (termino_correctamente):\n",
    "                print(f\"La eval. en el paso {paso} terminó en {pasos_eval} pasos.\")\n",
    "            else:\n",
    "                print(f\"En el paso {paso} se alcanzaron los pasos maximos en la eval.\")\n",
    "\n",
    "\n",
    "        # en este paso, ya tenemos nuestro V actualizado tras la evaluacion\n",
    "\n",
    "        politica_estable = True\n",
    "\n",
    "        for s in range(1, n_S-1):\n",
    "\n",
    "            a_mas_probable = policy[s].tolist().index(max(policy[s]))\n",
    "            # OJO, hay que pensar una manera de que cuando las p sean todas iguales, la cambie por 1.\n",
    "            #print(f\"state = {s}, a mas probable = {a_mas_probable}\")\n",
    "\n",
    "            acciones = []        \n",
    "            for a in env[s]:\n",
    "                retorno_a = 0\n",
    "                for transicion in env[s][a]:\n",
    "                    p_transicion = transicion[0]\n",
    "                    proximo_estado = transicion[1]\n",
    "                    reward = transicion[2]\n",
    "                    retorno_a += p_transicion*(reward + discount_factor*V[proximo_estado])\n",
    "                acciones.append((a, retorno_a))\n",
    "\n",
    "\n",
    "            mejor_a = max(acciones, key=lambda x: x[1])[0]\n",
    "\n",
    "            #diferencia = acciones[mejor_a][1] - acciones[a_mas_probable][1]\n",
    "            #diferencia_rel = (diferencia)/(acciones[a_mas_probable][1])\n",
    "            diferencia_rel = (acciones[mejor_a][1])/(acciones[a_mas_probable][1])\n",
    "\n",
    "            if (debug == 3):\n",
    "                print(f\"paso {paso} | s = {s} | a_priori = {a_mas_probable} | dsp = {mejor_a} | modifica= {diferencia_rel > 1}\")\n",
    "                print(f\"a_priori p = {acciones[a_mas_probable][1]} | dsp p = {acciones[mejor_a][1]} | dif rel = {diferencia_rel -1}\")\n",
    "                #print(acciones)\n",
    "                \n",
    "            if ((diferencia_rel > 1) and (mejor_a != a_mas_probable)):\n",
    "                if (debug==2):\n",
    "                    print(f\"paso {paso} | s = {s} | a_priori = {a_mas_probable} | dsp = {mejor_a} | dif = {diferencia_rel-1}\")\n",
    "                    #print(f\"a_priori p = {acciones[a_mas_probable][1]} | dsp p = {acciones[mejor_a][1]}\")\n",
    "\n",
    "                politica_estable = False\n",
    "                pol_s = np.zeros(51)\n",
    "                pol_s[mejor_a] = 1\n",
    "                policy[s] = pol_s\n",
    "\n",
    "\n",
    "        if (politica_estable):\n",
    "            print(f\"Se encontro una politica optima tras {paso} paso/s.\\n\")\n",
    "            break\n",
    "\n",
    "        if (max_steps_opt):\n",
    "            if (paso == max_steps_opt):\n",
    "                print(f\"Se corta la optimizacion de la politica al llegar a los pasos maximos ({max_steps_opt}).\\n\")\n",
    "                break\n",
    "\n",
    "        paso += 1\n",
    "    \n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontro una politica optima tras 5 paso/s.\n",
      "\n",
      "Política optimizada:\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Función de valor óptima:\n",
      "[0.00000000e+00 7.28611644e-05 2.91444667e-04 6.95264566e-04\n",
      " 1.16577869e-03 1.77125505e-03 2.78105828e-03 4.03661210e-03\n",
      " 4.66311477e-03 5.60141644e-03 7.08502024e-03 9.04088770e-03\n",
      " 1.11242331e-02 1.56796459e-02 1.61464484e-02 1.69534413e-02\n",
      " 1.86524591e-02 1.98260623e-02 2.24056658e-02 2.73847344e-02\n",
      " 2.83400810e-02 3.04945467e-02 3.61635508e-02 3.84959101e-02\n",
      " 4.44969325e-02 6.25000000e-02 6.27185835e-02 6.33743340e-02\n",
      " 6.45857937e-02 6.59973361e-02 6.78137652e-02 7.08431749e-02\n",
      " 7.46098363e-02 7.64893443e-02 7.93042493e-02 8.37550607e-02\n",
      " 8.96226631e-02 9.58726994e-02 1.09538938e-01 1.10939345e-01\n",
      " 1.13360324e-01 1.18457377e-01 1.21978187e-01 1.29716997e-01\n",
      " 1.44654203e-01 1.47520243e-01 1.53983640e-01 1.70990652e-01\n",
      " 1.77987730e-01 1.95990798e-01 2.50000000e-01 2.50218584e-01\n",
      " 2.50874334e-01 2.52085794e-01 2.53497336e-01 2.55313765e-01\n",
      " 2.58343175e-01 2.62109836e-01 2.63989344e-01 2.66804249e-01\n",
      " 2.71255061e-01 2.77122663e-01 2.83372699e-01 2.97038938e-01\n",
      " 2.98439345e-01 3.00860324e-01 3.05957377e-01 3.09478187e-01\n",
      " 3.17216997e-01 3.32154203e-01 3.35020243e-01 3.41483640e-01\n",
      " 3.58490652e-01 3.65487730e-01 3.83490798e-01 4.37500000e-01\n",
      " 4.38155751e-01 4.40123002e-01 4.43757381e-01 4.47992008e-01\n",
      " 4.53441296e-01 4.62529525e-01 4.73829509e-01 4.79468033e-01\n",
      " 4.87912748e-01 5.01265182e-01 5.18867989e-01 5.37618098e-01\n",
      " 5.78616813e-01 5.82818036e-01 5.90080972e-01 6.05372132e-01\n",
      " 6.15934561e-01 6.39150992e-01 6.83962610e-01 6.92560729e-01\n",
      " 7.11950921e-01 7.62971957e-01 7.83963191e-01 8.37972393e-01\n",
      " 0.00000000e+00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration_for_gamblers(p_h=0.25, theta_eval=1e-10, debug=0, max_steps_opt=20)\n",
    "\n",
    "print(\"Política optimizada:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Función de valor óptima:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontro una politica optima tras 9 paso/s.\n",
      "\n",
      "Política optimizada:\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Función de valor óptima:\n",
      "[0.         0.18181818 0.33057851 0.45229151 0.55187487 0.63335216\n",
      " 0.7000154  0.75455806 0.79918386 0.83569589 0.86556936 0.89001129\n",
      " 0.91000924 0.92637119 0.93975825 0.95071129 0.95967288 0.96700508\n",
      " 0.97300416 0.97791249 0.9819284  0.98521415 0.98790248 0.99010203\n",
      " 0.99190166 0.99337409 0.9945788  0.99556447 0.99637093 0.99703076\n",
      " 0.99757062 0.99801233 0.99837372 0.99866941 0.99891134 0.99910927\n",
      " 0.99927122 0.99940373 0.99951214 0.99960084 0.99967342 0.9997328\n",
      " 0.99978138 0.99982113 0.99985365 0.99988026 0.99990203 0.99991984\n",
      " 0.99993442 0.99994634 0.9999561  0.99996408 0.99997061 0.99997596\n",
      " 0.99998033 0.9999839  0.99998683 0.99998923 0.99999119 0.99999279\n",
      " 0.9999941  0.99999517 0.99999605 0.99999677 0.99999736 0.99999784\n",
      " 0.99999823 0.99999855 0.99999882 0.99999903 0.99999921 0.99999935\n",
      " 0.99999947 0.99999957 0.99999965 0.99999971 0.99999976 0.99999981\n",
      " 0.99999984 0.99999987 0.9999999  0.99999991 0.99999993 0.99999994\n",
      " 0.99999995 0.99999996 0.99999997 0.99999998 0.99999998 0.99999998\n",
      " 0.99999999 0.99999999 0.99999999 0.99999999 1.         1.\n",
      " 1.         1.         1.         1.         0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration_for_gamblers(p_h=0.55, theta_eval=1e-10, debug=0, max_steps_opt=20)\n",
    "\n",
    "print(\"Política optimizada:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Función de valor óptima:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotear la política final (apuesta) vs estado (capital)\n",
    "\n",
    "# Implementar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotear capital vs política final\n",
    "\n",
    "# Implementar!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
