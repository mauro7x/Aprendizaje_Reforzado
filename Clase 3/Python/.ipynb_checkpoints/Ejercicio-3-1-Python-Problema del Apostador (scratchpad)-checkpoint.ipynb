{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Este es el ejemplo 4.3. Problema del Apostador (Gambler’s Problem) del libro de Sutton. \n",
    "\n",
    "Un apostador tiene la oportunidad de hacer apuestas a los resultados de una secuencia de tiros de una moneda. \n",
    "Si la moneda cae cara, gana tantos dólares como apostó en esa tirada.\n",
    "Si la moneda cae ceca, pierde lo apostado. El juego termina cuando un apostador gana alcanzando su objetivo de $100, o pierde quedándose sin dinero.\n",
    "\n",
    "En cada tirada el apostador debe decidir qué porción de su capital quiere apostar, una cantidad entera de dólares.\n",
    "El problema puede ser formulado como un MDP finito sin descuento, episódico.\n",
    "\n",
    "Los estados posibles del capital del apostador son: s ∈ {1, 2, . . . , 99}.\n",
    "\n",
    "La acciones son apuestas,  a ∈ {0, 1, . . . , min(s, 100 − s)}. \n",
    "\n",
    "La recompensa es cero en todas las transiciones excepto en aquellas en que el apostador alcanza su objetivo, en que la recompensa es +1.\n",
    "\n",
    "La función de estado-valor da la probabilidad de ganar desde cada estado. Una política es una función de niveles de capital a apuestas. La política óptima maximiza la probabilidad de obtener el objetivo. Llamemos p_h la probabilidad de que una moneda salga cara. Si p_h es conocida, entonces el problema se puede resolver, por ejemplo, con iteración de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Implementar iteración de valor para el problema del apostador y resolverlo para p_h = 0.25 y p_h = 0.55.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration_for_gamblers(p_h, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p_h: Probabilidad de que una moneda caiga cara\n",
    "    \"\"\"\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    def one_step_lookahead(s, V, rewards):\n",
    "        \"\"\"\n",
    "        Función auxiliar que calcula el valor de todas las acciones dado un estado.\n",
    "        \n",
    "        Args:\n",
    "            s: El capital del apostador. Entero.\n",
    "            V: El vector que contiene los valores en cada estado.\n",
    "            rewards: El vector recompensa.\n",
    "                        \n",
    "        Returns:\n",
    "            Un vector que contiene el valor esperado de cada acción.\n",
    "            Su longitud es igual a la cantidad de acciones.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Implementar!\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    # Implementar!\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy, v = value_iteration_for_gamblers(0.25)\n",
    "\n",
    "print(\"Política optimizada:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Función de valor óptima:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = value_iteration_for_gamblers(0.55)\n",
    "\n",
    "print(\"Política optimizada:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Función de valor óptima:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotear la política final (apuesta) vs estado (capital)\n",
    "\n",
    "# Implementar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plotear capital vs política final\n",
    "\n",
    "# Implementar!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad\n",
    "En esta zona voy a ir armando los componentes necesarios para poder resolver el ejercicio, para luego pasarlo en limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constantes\n",
    "n_S = 101\n",
    "n_A = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empezamos con una random\n",
    "\n",
    "def policy_random(n_S=101, n_A=51):\n",
    "    policy = np.zeros([n_S, n_A])\n",
    "    \n",
    "    for s in range(51):\n",
    "        for a in range(0, s+1):\n",
    "            policy[s][a] = 1/(s+1)\n",
    "\n",
    "    for s in range(51,101):\n",
    "        n_a_posibles = min(s,100-s)+1\n",
    "        for a in range(n_a_posibles):\n",
    "            policy[s][a] = 1/(n_a_posibles)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion de valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V = np.zeros(n_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se respeta el formato utilizado en el ejercicio anterior, cada env[s][a] = lista de tuplas\n",
    "# con formato (probabilidad, sig_estado, reward, done)\n",
    "\n",
    "def reset_env(p=0.25):\n",
    "    env = {}\n",
    "\n",
    "    for s in range(0, 101):\n",
    "        env[s] = {}\n",
    "\n",
    "        for a in range(min(s,100-s)+1):        \n",
    "            env[s][a] = []\n",
    "\n",
    "            if (a==0):\n",
    "                r = 1.0 if (s==100) else 0.0\n",
    "                done = True if ((s==0) or (s==100)) else False\n",
    "                env[s][a] = [(1.0, s, r, done)]\n",
    "            else:\n",
    "                sig_estado_si_gano = s+a\n",
    "                sig_estado_si_pierdo = s-a\n",
    "                done_si_gano = True if (sig_estado_si_gano==100) else False\n",
    "                done_si_pierdo = True if (sig_estado_si_pierdo==0) else False\n",
    "                r = 1.0 if (sig_estado_si_gano==100) else 0.0\n",
    "                env[s][a] = [(p, sig_estado_si_gano, r, done_si_gano), ((1-p), (s-a), 0.0, done_si_pierdo)]\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_eval\n",
    "\n",
    "def policy_eval(policy, env, n_S=101, discount_factor=1.0, theta=1e-05, n_max = 0):\n",
    "    \n",
    "    pasos = 0\n",
    "    V = np.zeros(n_S)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(1, n_S-1):\n",
    "            v = 0\n",
    "            for a in env[s]:\n",
    "                for  prob, next_state, reward, done in env[s][a]:\n",
    "                    v += policy[s][a] * prob * (reward + discount_factor * V[next_state])\n",
    "            \n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            \n",
    "            # for debug:\n",
    "            #print(f\"Paso {pasos+1} | Estado {s} | V_previo = {V[s]}, V_nuevo = {v}\")\n",
    "            \n",
    "            \n",
    "            V[s] = v\n",
    "                \n",
    "        pasos += 1\n",
    "        \n",
    "        if delta < theta:\n",
    "            termino_correctamente = True\n",
    "            break\n",
    "            \n",
    "        if (n_max):\n",
    "            if (pasos == n_max):\n",
    "                termino_correctamente = False\n",
    "                break\n",
    "    \n",
    "    if (termino_correctamente):\n",
    "        print(f\"La eval. termino en {pasos} pasos.\")\n",
    "    else:\n",
    "        print(f\"Se alcanzaron los pasos maximos.\")\n",
    "   \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, theta_opt=1e-08, theta_eval=1e-05, n_S=101, policy_eval_fn=policy_eval,\n",
    "                       policy_seed_fn=policy_random, discount_factor=1.0, n_max=0, debug=False):\n",
    "    \n",
    "    pasos = 0\n",
    "    \n",
    "    # Comenzar con política aleatoria\n",
    "    policy = policy_seed_fn()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "                \n",
    "        V = policy_eval_fn(policy, env, theta= theta_eval, n_S=n_S, discount_factor=discount_factor)\n",
    "                    \n",
    "        politica_estable = True\n",
    "        \n",
    "        for s in range(1, n_S-1):\n",
    "                        \n",
    "            a_mas_probable = policy[s].tolist().index(max(policy[s]))\n",
    "            # OJO, hay que pensar una manera de que cuando las p sean todas iguales, la cambie por 1.\n",
    "            \n",
    "            #print(f\"state = {s}, a mas probable = {a_mas_probable}\")\n",
    "            \n",
    "            acciones = []        \n",
    "            for a in env[s]:\n",
    "                retorno_a = 0\n",
    "                for transicion in env[s][a]:\n",
    "                    p_transicion = transicion[0]\n",
    "                    proximo_estado = transicion[1]\n",
    "                    reward = transicion[2]\n",
    "                    retorno_a += p_transicion*(reward + discount_factor*V[proximo_estado])\n",
    "                acciones.append((a, retorno_a))\n",
    "\n",
    "\n",
    "            mejor_a = max(acciones, key=lambda x: x[1])[0]\n",
    "\n",
    "            \n",
    "            # si la acción de la política actual no coincide con la mejor calculada\n",
    "            # actualizar la política\n",
    "            # marcar que la política no fue estable en este paso\n",
    "            diferencia = acciones[mejor_a][1] - acciones[a_mas_probable][1]\n",
    "            #if ((diferencia > tetha) and (mejor_a != 0)):\n",
    "                    \n",
    "            if (diferencia > theta_opt):\n",
    "                if (debug):\n",
    "                    print(f\"paso {pasos+1} | s = {s} | a_priori = {a_mas_probable} | dsp = {mejor_a}\")\n",
    "                    print(f\"a_priori p = {acciones[a_mas_probable][1]} | dsp p = {acciones[mejor_a][1]}\")\n",
    "                politica_estable = False\n",
    "                pol_s = np.zeros(51)\n",
    "                pol_s[mejor_a] = 1\n",
    "                policy[s] = pol_s\n",
    "   \n",
    "        # si la política es estable, devolver la política óptima y la función de valor de esa política\n",
    "        if (politica_estable):\n",
    "            print(\"Se encontro una politica optima.\")\n",
    "            return policy, V\n",
    "        \n",
    "        pasos += 1\n",
    "        \n",
    "        if (n_max):\n",
    "            if (pasos == n_max):\n",
    "                print(\"Se corta la optimizacion de la politica al llegar a los pasos maximos.\")\n",
    "                return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeamos si funciona..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La eval. termino en 22 pasos.\n",
      "La eval. termino en 10 pasos.\n",
      "La eval. termino en 10 pasos.\n",
      "Se encontro una politica optima.\n"
     ]
    }
   ],
   "source": [
    "env = reset_env(p=0.25)\n",
    "policy, v = policy_improvement(env, n_max=15, theta_eval=1e-3, theta_opt=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política optimizada:\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "Función de valor óptima:\n",
      "[0.00000000e+00 0.00000000e+00 1.83105469e-04 6.07490540e-04\n",
      " 1.15966797e-03 1.60217285e-03 2.70247459e-03 3.90625000e-03\n",
      " 4.63867188e-03 5.59997559e-03 7.01618195e-03 9.03320312e-03\n",
      " 1.11241192e-02 1.56250000e-02 1.60806179e-02 1.68266296e-02\n",
      " 1.85546875e-02 1.98249817e-02 2.23999023e-02 2.73437500e-02\n",
      " 2.82449722e-02 3.04937363e-02 3.61328125e-02 3.84953022e-02\n",
      " 4.44964767e-02 6.25000000e-02 6.26373291e-02 6.33697510e-02\n",
      " 6.45268559e-02 6.59790039e-02 6.77621365e-02 7.08430894e-02\n",
      " 7.45604634e-02 7.64160156e-02 7.92999268e-02 8.36837292e-02\n",
      " 8.95996094e-02 9.58723575e-02 1.09477997e-01 1.10895142e-01\n",
      " 1.13321602e-01 1.18420348e-01 1.21974945e-01 1.29699707e-01\n",
      " 1.44608498e-01 1.47491202e-01 1.53981209e-01 1.70956373e-01\n",
      " 1.77985907e-01 1.95989430e-01 2.50000000e-01 2.50137329e-01\n",
      " 2.50869751e-01 2.52026856e-01 2.53479004e-01 2.55262136e-01\n",
      " 2.58343089e-01 2.62060463e-01 2.63916016e-01 2.66799927e-01\n",
      " 2.71183729e-01 2.77099609e-01 2.83372357e-01 2.96977997e-01\n",
      " 2.98395142e-01 3.00821602e-01 3.05920348e-01 3.09474945e-01\n",
      " 3.17199707e-01 3.32108498e-01 3.34991202e-01 3.41481209e-01\n",
      " 3.58456373e-01 3.65485907e-01 3.83489430e-01 4.37500000e-01\n",
      " 4.38152313e-01 4.40109253e-01 4.43757317e-01 4.47937012e-01\n",
      " 4.53387797e-01 4.62529268e-01 4.73796356e-01 4.79440261e-01\n",
      " 4.87899780e-01 5.01243401e-01 5.18842280e-01 5.37617072e-01\n",
      " 5.78614235e-01 5.82817988e-01 5.90040848e-01 6.05347267e-01\n",
      " 6.15924835e-01 6.39131710e-01 6.83960676e-01 6.92530636e-01\n",
      " 7.11943626e-01 7.62970507e-01 7.83957720e-01 8.37968290e-01\n",
      " 0.00000000e+00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Política optimizada:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Función de valor óptima:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si tenes 1 dls, tenes que apostar 0. Tenes una prob de ganar del 0.0%.\n",
      "Si tenes 2 dls, tenes que apostar 0. Tenes una prob de ganar del 0.02%.\n",
      "Si tenes 3 dls, tenes que apostar 0. Tenes una prob de ganar del 0.04%.\n",
      "Si tenes 4 dls, tenes que apostar 0. Tenes una prob de ganar del 0.09%.\n",
      "Si tenes 5 dls, tenes que apostar 0. Tenes una prob de ganar del 0.14%.\n",
      "Si tenes 6 dls, tenes que apostar 0. Tenes una prob de ganar del 0.21%.\n",
      "Si tenes 7 dls, tenes que apostar 7. Tenes una prob de ganar del 0.4%.\n",
      "Si tenes 8 dls, tenes que apostar 8. Tenes una prob de ganar del 0.47%.\n",
      "Si tenes 9 dls, tenes que apostar 9. Tenes una prob de ganar del 0.56%.\n",
      "Si tenes 10 dls, tenes que apostar 10. Tenes una prob de ganar del 0.71%.\n",
      "Si tenes 11 dls, tenes que apostar 11. Tenes una prob de ganar del 0.9%.\n",
      "Si tenes 12 dls, tenes que apostar 12. Tenes una prob de ganar del 1.11%.\n",
      "Si tenes 13 dls, tenes que apostar 13. Tenes una prob de ganar del 1.57%.\n",
      "Si tenes 14 dls, tenes que apostar 14. Tenes una prob de ganar del 1.61%.\n",
      "Si tenes 15 dls, tenes que apostar 15. Tenes una prob de ganar del 1.7%.\n",
      "Si tenes 16 dls, tenes que apostar 16. Tenes una prob de ganar del 1.87%.\n",
      "Si tenes 17 dls, tenes que apostar 17. Tenes una prob de ganar del 1.98%.\n",
      "Si tenes 18 dls, tenes que apostar 18. Tenes una prob de ganar del 2.24%.\n",
      "Si tenes 19 dls, tenes que apostar 19. Tenes una prob de ganar del 2.74%.\n",
      "Si tenes 20 dls, tenes que apostar 20. Tenes una prob de ganar del 2.83%.\n",
      "Si tenes 21 dls, tenes que apostar 21. Tenes una prob de ganar del 3.05%.\n",
      "Si tenes 22 dls, tenes que apostar 22. Tenes una prob de ganar del 3.62%.\n",
      "Si tenes 23 dls, tenes que apostar 23. Tenes una prob de ganar del 3.85%.\n",
      "Si tenes 24 dls, tenes que apostar 24. Tenes una prob de ganar del 4.45%.\n",
      "Si tenes 25 dls, tenes que apostar 25. Tenes una prob de ganar del 6.25%.\n",
      "Si tenes 26 dls, tenes que apostar 26. Tenes una prob de ganar del 6.27%.\n",
      "Si tenes 27 dls, tenes que apostar 27. Tenes una prob de ganar del 6.34%.\n",
      "Si tenes 28 dls, tenes que apostar 28. Tenes una prob de ganar del 6.46%.\n",
      "Si tenes 29 dls, tenes que apostar 29. Tenes una prob de ganar del 6.6%.\n",
      "Si tenes 30 dls, tenes que apostar 30. Tenes una prob de ganar del 6.78%.\n",
      "Si tenes 31 dls, tenes que apostar 31. Tenes una prob de ganar del 7.08%.\n",
      "Si tenes 32 dls, tenes que apostar 32. Tenes una prob de ganar del 7.46%.\n",
      "Si tenes 33 dls, tenes que apostar 33. Tenes una prob de ganar del 7.65%.\n",
      "Si tenes 34 dls, tenes que apostar 34. Tenes una prob de ganar del 7.93%.\n",
      "Si tenes 35 dls, tenes que apostar 35. Tenes una prob de ganar del 8.38%.\n",
      "Si tenes 36 dls, tenes que apostar 36. Tenes una prob de ganar del 8.96%.\n",
      "Si tenes 37 dls, tenes que apostar 37. Tenes una prob de ganar del 9.59%.\n",
      "Si tenes 38 dls, tenes que apostar 38. Tenes una prob de ganar del 10.95%.\n",
      "Si tenes 39 dls, tenes que apostar 39. Tenes una prob de ganar del 11.09%.\n",
      "Si tenes 40 dls, tenes que apostar 40. Tenes una prob de ganar del 11.34%.\n",
      "Si tenes 41 dls, tenes que apostar 41. Tenes una prob de ganar del 11.85%.\n",
      "Si tenes 42 dls, tenes que apostar 42. Tenes una prob de ganar del 12.2%.\n",
      "Si tenes 43 dls, tenes que apostar 43. Tenes una prob de ganar del 12.97%.\n",
      "Si tenes 44 dls, tenes que apostar 44. Tenes una prob de ganar del 14.46%.\n",
      "Si tenes 45 dls, tenes que apostar 45. Tenes una prob de ganar del 14.75%.\n",
      "Si tenes 46 dls, tenes que apostar 46. Tenes una prob de ganar del 15.4%.\n",
      "Si tenes 47 dls, tenes que apostar 47. Tenes una prob de ganar del 17.1%.\n",
      "Si tenes 48 dls, tenes que apostar 48. Tenes una prob de ganar del 17.8%.\n",
      "Si tenes 49 dls, tenes que apostar 49. Tenes una prob de ganar del 19.6%.\n",
      "Si tenes 50 dls, tenes que apostar 50. Tenes una prob de ganar del 25.0%.\n",
      "Si tenes 51 dls, tenes que apostar 49. Tenes una prob de ganar del 25.01%.\n",
      "Si tenes 52 dls, tenes que apostar 48. Tenes una prob de ganar del 25.06%.\n",
      "Si tenes 53 dls, tenes que apostar 47. Tenes una prob de ganar del 25.16%.\n",
      "Si tenes 54 dls, tenes que apostar 4. Tenes una prob de ganar del 25.35%.\n",
      "Si tenes 55 dls, tenes que apostar 5. Tenes una prob de ganar del 25.53%.\n",
      "Si tenes 56 dls, tenes que apostar 6. Tenes una prob de ganar del 25.83%.\n",
      "Si tenes 57 dls, tenes que apostar 43. Tenes una prob de ganar del 26.21%.\n",
      "Si tenes 58 dls, tenes que apostar 42. Tenes una prob de ganar del 26.4%.\n",
      "Si tenes 59 dls, tenes que apostar 41. Tenes una prob de ganar del 26.68%.\n",
      "Si tenes 60 dls, tenes que apostar 40. Tenes una prob de ganar del 27.13%.\n",
      "Si tenes 61 dls, tenes que apostar 39. Tenes una prob de ganar del 27.71%.\n",
      "Si tenes 62 dls, tenes que apostar 38. Tenes una prob de ganar del 28.34%.\n",
      "Si tenes 63 dls, tenes que apostar 37. Tenes una prob de ganar del 29.7%.\n",
      "Si tenes 64 dls, tenes que apostar 36. Tenes una prob de ganar del 29.84%.\n",
      "Si tenes 65 dls, tenes que apostar 35. Tenes una prob de ganar del 30.09%.\n",
      "Si tenes 66 dls, tenes que apostar 34. Tenes una prob de ganar del 30.6%.\n",
      "Si tenes 67 dls, tenes que apostar 33. Tenes una prob de ganar del 30.95%.\n",
      "Si tenes 68 dls, tenes que apostar 32. Tenes una prob de ganar del 31.72%.\n",
      "Si tenes 69 dls, tenes que apostar 31. Tenes una prob de ganar del 33.21%.\n",
      "Si tenes 70 dls, tenes que apostar 30. Tenes una prob de ganar del 33.5%.\n",
      "Si tenes 71 dls, tenes que apostar 29. Tenes una prob de ganar del 34.15%.\n",
      "Si tenes 72 dls, tenes que apostar 28. Tenes una prob de ganar del 35.85%.\n",
      "Si tenes 73 dls, tenes que apostar 27. Tenes una prob de ganar del 36.55%.\n",
      "Si tenes 74 dls, tenes que apostar 26. Tenes una prob de ganar del 38.35%.\n",
      "Si tenes 75 dls, tenes que apostar 25. Tenes una prob de ganar del 43.75%.\n",
      "Si tenes 76 dls, tenes que apostar 24. Tenes una prob de ganar del 43.8%.\n",
      "Si tenes 77 dls, tenes que apostar 23. Tenes una prob de ganar del 44.01%.\n",
      "Si tenes 78 dls, tenes que apostar 3. Tenes una prob de ganar del 44.38%.\n",
      "Si tenes 79 dls, tenes que apostar 21. Tenes una prob de ganar del 44.8%.\n",
      "Si tenes 80 dls, tenes que apostar 20. Tenes una prob de ganar del 45.34%.\n",
      "Si tenes 81 dls, tenes que apostar 19. Tenes una prob de ganar del 46.25%.\n",
      "Si tenes 82 dls, tenes que apostar 18. Tenes una prob de ganar del 47.38%.\n",
      "Si tenes 83 dls, tenes que apostar 17. Tenes una prob de ganar del 47.95%.\n",
      "Si tenes 84 dls, tenes que apostar 16. Tenes una prob de ganar del 48.79%.\n",
      "Si tenes 85 dls, tenes que apostar 15. Tenes una prob de ganar del 50.13%.\n",
      "Si tenes 86 dls, tenes que apostar 14. Tenes una prob de ganar del 51.88%.\n",
      "Si tenes 87 dls, tenes que apostar 13. Tenes una prob de ganar del 53.76%.\n",
      "Si tenes 88 dls, tenes que apostar 12. Tenes una prob de ganar del 57.85%.\n",
      "Si tenes 89 dls, tenes que apostar 11. Tenes una prob de ganar del 58.28%.\n",
      "Si tenes 90 dls, tenes que apostar 10. Tenes una prob de ganar del 59.01%.\n",
      "Si tenes 91 dls, tenes que apostar 9. Tenes una prob de ganar del 60.54%.\n",
      "Si tenes 92 dls, tenes que apostar 8. Tenes una prob de ganar del 61.59%.\n",
      "Si tenes 93 dls, tenes que apostar 7. Tenes una prob de ganar del 63.91%.\n",
      "Si tenes 94 dls, tenes que apostar 6. Tenes una prob de ganar del 68.39%.\n",
      "Si tenes 95 dls, tenes que apostar 5. Tenes una prob de ganar del 69.26%.\n",
      "Si tenes 96 dls, tenes que apostar 4. Tenes una prob de ganar del 71.19%.\n",
      "Si tenes 97 dls, tenes que apostar 3. Tenes una prob de ganar del 76.29%.\n",
      "Si tenes 98 dls, tenes que apostar 2. Tenes una prob de ganar del 78.4%.\n",
      "Si tenes 99 dls, tenes que apostar 0. Tenes una prob de ganar del 83.71%.\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for estado in policy:\n",
    "    idx = estado.tolist().index(max(estado))\n",
    "    if ((s != 0) and (s != 100)):\n",
    "        print(f\"Si tenes {s} dls, tenes que apostar {idx}. Tenes una prob de ganar del {round(100*v[s], 2)}%.\")\n",
    "        #print(f\"Si tenes {s} dls, tenes que apostar {idx}. Tenes una prob de ganar del {100*v[s]}%.\")\n",
    "    s+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
